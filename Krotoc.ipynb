{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a201a0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection\n",
    "import sklearn\n",
    "import torch.optim as Adam\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "\n",
    "digits = sklearn.datasets.load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "X = X /16\n",
    "x_train, x_test,y_train, y_test = sklearn.model_selection.train_test_split(X, y, \n",
    "                                    test_size = .2, random_state = 47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "3dd298c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "peekX = 10\n",
    "peekY = 10\n",
    "dim = 2000\n",
    "dim = max(dim, peekX*peekY)\n",
    "Nepochs = 2000\n",
    "batch_size = 100\n",
    "epsmax = .003\n",
    "idim = np.shape(X[0])[0]\n",
    "odim = 10\n",
    "datx = 8\n",
    "daty = 8\n",
    "p = 3\n",
    "k = 4\n",
    "delta = .4\n",
    "n = 4.5\n",
    "prec = 1e-30\n",
    "R=1\n",
    "train_size = np.shape(x_train)[0]\n",
    "test_size = np.shape(x_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f65a944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def look(X):\n",
    "    plt.gray()\n",
    "    plt.matshow(np.asmatrix(np.reshape(X, (8,8))))\n",
    "    plt.show()\n",
    "#look(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ca39d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek_weights(datx, daty, peekX, peekY, weights):\n",
    "    fig=plt.figure(figsize=(12.9,10))\n",
    "    fulmat = np.zeros((datx*peekX,daty*peekY))\n",
    "    for i in range(peekX):\n",
    "        for j in range(peekY):\n",
    "            fulmat[i*datx:i*datx+datx,j*daty:j*daty+daty] = np.reshape(weights[peekY*i+j], (datx,daty))\n",
    "    im = plt.imshow(fulmat, cmap = 'bwr')\n",
    "    fig.colorbar(im,ticks=[np.amin(fulmat), 0, np.amax(fulmat)])\n",
    "    plt.axis('off')\n",
    "    return\n",
    "\n",
    "def vec_prod(x, w, p):\n",
    "    return np.dot(np.sign(w)*np.absolute(w)**(p-1), np.transpose(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c5204cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsuperv_fast(x_train, y_train):\n",
    "    weights = np.random.normal(0, 1, (dim, idim))\n",
    "    for epochs in range(Nepochs):\n",
    "        eps = epsmax*(1-epochs/Nepochs)\n",
    "        x_train, y_train = sklearn.utils.shuffle(x_train, y_train)\n",
    "        for i in range (train_size//batch_size):\n",
    "            x_batch = x_train[batch_size*i:batch_size*(i+1),:]\n",
    "            inp_lay = vec_prod(x_batch, weights, p)\n",
    "            #Fast implementation\n",
    "            argkey = np.argsort(inp_lay, axis = 0)\n",
    "            gfunc = np.zeros((dim, batch_size))\n",
    "            gfunc[argkey[idim-1,:], np.arange(batch_size)]=1\n",
    "            gfunc[argkey[idim-1-k,:], np.arange(batch_size)]= -1*delta\n",
    "            gim = np.sum(np.multiply(gfunc,inp_lay), axis = 1)\n",
    "            v = R**p*np.dot(gfunc, x_batch)-np.multiply(np.outer(gim, np.ones(idim)), weights)\n",
    "            \n",
    "            trans = sklearn.preprocessing.MaxAbsScaler()\n",
    "            trans.fit(v)\n",
    "            weights += eps*trans.transform(v)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "766f8083",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-63-491e58ea61ca>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-63-491e58ea61ca>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    x_train, y_train = sklearn.utils.shuffle(x_train, y_train, random_state = 47)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# TODO: finish supervised learning algorithm\n",
    "def learn_final_layer(weights):\n",
    "    finlay = np.random.normal((dim, odim))\n",
    "    for epochs in range(Nepochs):\n",
    "    \n",
    "    x_train, y_train = sklearn.utils.shuffle(x_train, y_train, random_state = 47)\n",
    "    for i in range (train_size//batch_size):\n",
    "        x_batch = x_train[batch_size*i:batch_size*(i+1),:]\n",
    "        inp_lay = vec_prod(x_batch, weights, weights, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0c6af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = unsuperv_fast(x_train, y_train)\n",
    "peek_weights(datx, daty, peekX, peekY, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ab7468",
   "metadata": {},
   "source": [
    "I'm thinking one we have hyperparameters tuned and everything we can finsh the learning the final layer code, and maybe tru to learn multiple layers biologically inspired. Idk how much we should do... also I guess we can do some standard backprop trained neural net too. Not sure if there's anything else to do that seems like a lot as is."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
